#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sat Feb 10 00:34:16 2018

@author: cxue1
"""

"""
A simple wrapper for linear regression.  (c) 2015 Tucker Balch
"""

import numpy as np
import RTLearner as rt
import DTLearner as dt
import BagLearner as bl
import LinRegLearner as lrl

#insaneLearner should contain 20 BagLearner instances 
#each instance is composed of 20 LinRegLearner instances
class InsaneLearner(object):
    def __init__(self, verbose = False):
        pass
    def author(self):
        return 'cxue34'
    
    def addEvidence(self, dataX, dataY):
        self.dataX=dataX
        self.dataY=dataY
        
    def query(self, test_dataX):
        all_Y=[]
        learner_bl=bl.BagLearner(learner = lrl.LinRegLearner, kwargs={}, bags = 20, boost = False, verbose = False)
        bag_learners=[]
        for i in range(0, 20):
            bag_learners.append(learner_bl)
        for lr in bag_learners:
            lr.addEvidence(self.dataX, self.dataY)
            trainY = lr.query(test_dataX)
            all_Y.append(trainY)
        predY=np.mean(np.array(all_Y), axis=0).tolist()
        return predY
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Fri Feb  9 23:46:57 2018

@author: cxue1
"""

"""
A simple wrapper for linear regression.  (c) 2015 Tucker Balch
"""

import numpy as np
class RTLearner(object):
    def __init__(self, leaf_size = 1, verbose = False): 
        self.leaf_size = leaf_size
        self.verbose = verbose
    def author(self):
        return 'cxue34' 
    def build_tree(self, dataX, dataY):
        if dataX.shape[0] <= self.leaf_size or len(np.unique(dataX[:,-1]))==1:
            return np.array([-1, dataY.mean(), np.nan, np.nan])
        else:
            col_list=dataX.shape[1]
            random_cols = np.random.randint(col_list,size=2)
            best_column = int(random_cols.mean()+0.5)
            SplitVal=np.median(dataX[:, best_column])                
            if (SplitVal == np.amax(dataX[:, best_column])) or (SplitVal == np.amin(dataX[:, best_column])):
                return np.array([-1, dataY.mean(), np.nan, np.nan])
            split_condition1=dataX[:, best_column]<=SplitVal
            split_condition2=dataX[:, best_column]>SplitVal        
            lefttree=self.build_tree(dataX[split_condition1], dataY[split_condition1])
            righttree=self.build_tree(dataX[split_condition2], dataY[split_condition2])
            lefttree_size = lefttree.ndim
            if lefttree_size > 1:
                self.root=np.array([best_column, SplitVal,1, lefttree.shape[0]+1])
            elif lefttree_size == 1:
                self.root = np.array([best_column, SplitVal,1, 2])
            return np.vstack((self.root, lefttree, righttree))
    def addEvidence(self,dataX,dataY):
        """
        @summary: Add training data to learner
        @param dataX: X values of data to add
        @param dataY: the Y training values
        """
        # slap on 1s column so linear regression finds a constant term
        self.tree = self.build_tree(dataX, dataY)
        if self.verbose == True:
            print(self.tree)
    def query(self,dataX_test):
        """
        @summary: Estimate a set of test points given the model we built.
        @param points: should be a numpy array with each row corresponding to a specific query.
        @returns the estimated values according to the saved model.
        """
        trainY = []
        for row in dataX_test:
            i = 0
            while (i < self.tree.shape[0]):
                feature_ind = int(self.tree[i, 0])
                # Found leaf
                if feature_ind == -1:
                    break
                #Judge the data should go to left tree or right tree
                if row[feature_ind] > self.tree[i, 1]:
                    i += int(self.tree[i, 3])
                elif row[feature_ind] <= self.tree[i, 1]:
                    i += 1
            if feature_ind >= 0:
                trainY.append(np.nan)
            else:
                trainY.append(self.tree[i, 1])
        return trainY
if __name__=="__main__":
    print "the secret clue is 'zzyzx'"

#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Fri Feb  9 23:46:22 2018

@author: cxue1
"""

"""
A simple wrapper for linear regression.  (c) 2015 Tucker Balch
"""
import RTLearner as rt
import DTLearner as dt
import LinRegLearner as lrl
import numpy as np
import random
class BagLearner(object):
    def __init__(self, learner = dt.DTLearner, kwargs={"argument1":1, "argument2":2}, bags = 20, boost = False, verbose = False):
        self.learner=learner
        self.kwargs = kwargs
        self.bags = bags
        self.boost = boost
        self.verbose = verbose
        self.learners = []
        for i in range(0, self.bags):
            self.learners.append(self.learner(**self.kwargs))
    def author(self):
        return 'cxue34'    
    def addEvidence(self, dataX, dataY):
        self.dataX=dataX
        self.dataY=dataY
    def query(self, test_dataX):
        all_Y=[]
        for lr in self.learners:
            data_size=self.dataX.shape[0]
            rand_ind=np.random.choice(data_size, data_size)
            rand_dataX=np.array([self.dataX.tolist()[i] for i in rand_ind])
            rand_dataY=np.array([self.dataY.tolist()[i] for i in rand_ind])
            lr.addEvidence(rand_dataX, rand_dataY)
            trainY = lr.query(test_dataX)
            all_Y.append(trainY)
        predY=np.mean(np.array(all_Y), axis=0).tolist()
        return predY
   



"""
A simple wrapper for linear regression.  (c) 2015 Tucker Balch
"""

import numpy as np
class DTLearner(object):
    def __init__(self, leaf_size = 1, verbose = False): 
        self.leaf_size = leaf_size
        self.verbose = verbose
    def author(self):
        return 'cxue34' 
    def build_tree(self, dataX, dataY):
        if dataX.shape[0] <= self.leaf_size or len(np.unique(dataX[:,-1]))==1:
            return np.array([-1, dataY.mean(), np.nan, np.nan])
        else:
            num_X=dataX.shape[1]
            corr_of_columns=[abs(np.corrcoef(dataX[:, i],dataY)[0,1]) for i in range(num_X)]
            best_column=np.nanargmax(corr_of_columns)
            SplitVal=np.median(dataX[:, best_column])                
            if (SplitVal == np.amax(dataX[:, best_column])) or (SplitVal == np.amin(dataX[:, best_column])):
                return np.array([-1, dataY.mean(), np.nan, np.nan])
            split_condition1=dataX[:, best_column]<=SplitVal
            split_condition2=dataX[:, best_column]>SplitVal  
            lefttree=self.build_tree(dataX[split_condition1], dataY[split_condition1])
            righttree=self.build_tree(dataX[split_condition2], dataY[split_condition2])
            lefttree_size = lefttree.ndim
            if lefttree_size > 1:
                self.root=np.array([best_column, SplitVal,1, lefttree.shape[0]+1])
            elif lefttree_size == 1:
                self.root = np.array([best_column, SplitVal,1, 2])
            return np.vstack((self.root, lefttree, righttree))
    def addEvidence(self,dataX,dataY):
        """
        @summary: Add training data to learner
        @param dataX: X values of data to add
        @param dataY: the Y training values
        """
        # slap on 1s column so linear regression finds a constant term
        self.tree = self.build_tree(dataX, dataY)
        if self.verbose == True:
            print(self.tree)        
    def query(self,dataX_test):
        """
        @summary: Estimate a set of test points given the model we built.
        @param points: should be a numpy array with each row corresponding to a specific query.
        @returns the estimated values according to the saved model.
        """
        trainY = []
        for row in dataX_test:
            i = 0
            while (i < self.tree.shape[0]):
                feature_ind = int(self.tree[i, 0])
                # Found leaf
                if feature_ind == -1:
                    break
                #Judge the data should go to left tree or right tree
                if row[feature_ind] > self.tree[i, 1]:
                    i += int(self.tree[i, 3])
                elif row[feature_ind] <= self.tree[i, 1]:
                    i += 1
            if feature_ind >= 0:
                trainY.append(np.nan)
            else:
                trainY.append(self.tree[i, 1])
        return trainY
if __name__=="__main__":
    print "the secret clue is 'zzyzx'"
